<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Homepage | Jongseo Lee</title>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600&family=Poppins:wght@300;400;600&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="assets/css/style.css">
</head>
<body>

    <header class="site-header">
        <h1>ğŸš€ Research Homepage | Jongseo Lee</h1>
    </header>

    <!-- Navigation Bar -->
    <nav class="navbar">
        <a href="#home">Home</a>
        <a href="#publications">Publications</a>
        <a href="#about">About</a>
        <a href="#contact">Contact</a>
    </nav>

    <!-- Home Section -->
    <div class="container" id="home">

    </div>
<!-- About Me Section -->
<!-- About Me Section -->
<!-- About Me Section -->
<div class="container" id="about">
    <h2>ğŸ§‘â€ğŸ’» About Me</h2>

    <div class="about-container">
        <!-- í”„ë¡œí•„ ì‚¬ì§„ -->
        <img src="assets/img/profile.jpg" alt="Jongseo Lee" class="profile-img">
        
        <!-- ì•½ë ¥ (Brief Bio) -->
        <div class="bio-info">
            <h3>Jongseo Lee</h3>
            <p><strong>ğŸ“ M.S. Student in Computer Science</strong></p>
            <p><strong>ğŸ› Kyung Hee University</strong></p>
            <p>ğŸ”¬ <a href="https://vll.khu.ac.kr/pub.html" target="_blank">Vision and Learning Lab</a>, Advised by Prof. Jinwoo Choi</p>
            <p>ğŸ“§ jong980812@khu.ac.kr</p>

            <!-- í•™ë ¥ ì¶”ê°€ -->
            <h4>ğŸ“ Education</h4>
            <ul>
                <li>ğŸ“Œ <strong>B.S. in Biomedical Engineering</strong>, Kyung Hee University (2017 â€“ 2023.2)</li>
                <li>ğŸ“Œ <strong>B.S. in Electronics Engineering</strong>, Kyung Hee University (2020 â€“ 2023.7)</li>
                <li>ğŸ“Œ <strong>M.S. in Computer Science</strong>, Kyung Hee University (2023.8 â€“ Present)</li>
            </ul>
        </div>
    </div>

    <!-- ìê¸°ì†Œê°œ -->
    <div class="about-description">
        <p>Hello! I am currently pursuing a <strong>Masterâ€™s degree in Computer Science</strong> at <strong>Kyung Hee University</strong>, with one semester remaining until graduation.</p>

        <p>Prior to my graduate studies, I earned <strong>dual bachelor's degrees</strong> in <strong>Biomedical Engineering</strong> and <strong>Electronics Engineering</strong>.  
        This interdisciplinary background has given me a strong foundation in <em>signal processing, embedded systems, and AI</em>, which I now integrate into my research.</p>

        <p>My research is conducted at the <strong>Vision and Learning Lab</strong> under <strong>Professor Jinwoo Choi</strong>, where I specialize in <strong>Video Understanding</strong>.  
        I am particularly focused on:</p>
        
        <ul>
            <li>ğŸ¥ <strong>Video Action Recognition</strong> â€“ Developing AI models that can accurately interpret human actions in videos.</li>
            <li>ğŸ“ <strong>Video-Text Multimodal Learning</strong> â€“ Integrating textual and visual information for richer video understanding.</li>
            <li>ğŸ” <strong>Explainable AI (XAI) for Video Analysis</strong> â€“ Making AI decisions more interpretable and trustworthy.</li>
        </ul>

        <!-- <p>In 2023, I was honored to have my work, <em>â€œCAST: Cross-Attention in Space and Time for Video Action Recognition,â€</em> accepted at <strong>NeurIPS 2023</strong> as the first author.  
        Building upon this, my current research focuses on <strong>multimodal learning approaches</strong> that fuse language and video data,  
        as well as <strong>improving explainability in AI-driven video analysis</strong>.</p> -->

        <p>Recent, I am deeply passionate about <strong>Video-eXplainable-AI </strong> and <strong> Video-Text multimodal learning </strong>.  
        I am always open to collaboration and discussions on cutting-edge research in <strong>Computer Vision, Multimodal AI, and Explainability</strong>.</p>

        <p><strong>Feel free to connect with me! ğŸš€</strong></p>
    </div>
</div>



    <div class="container" id="contact">
        <h2>âœ‰ï¸ Contact</h2>
    
        <div class="contact-card">
            <img src="https://cdn-icons-png.flaticon.com/512/561/561127.png" alt="Email Icon">
            <p><strong>Email:</strong> jong980812@khu.ac.kr</p>
        </div>
    
        <div class="contact-card">
            <img src="https://cdn-icons-png.flaticon.com/512/25/25231.png" alt="GitHub Icon">
            <p> <a href="https://github.com/jong980812" target="_blank">GitHub</a></p>
        </div>
    
        <div class="contact-card">
            <img src="https://cdn-icons-png.flaticon.com/512/174/174857.png" alt="LinkedIn Icon">
            <p> <a href="https://www.linkedin.com/in/jongseo-lee-a551ab244/" target="_blank">LinkedIn</a></p>
        </div>
    </div>
    
    

    <!-- Publications Section -->
    <div class="container" id="publications">
        <h2>ğŸ“„ Publications</h2>
        <div class="paper-card">
            <h3>Metaverse Interface with Haptic and Rigid Sense Feedback at a Low Cost</h3>
            <p><strong>Authors:</strong> <b>Jongseo Lee</b>, Su Hyeon Kim, Sun Woong Jang, Jun Yeong Moon, Doug Young Suh</p>
            <p><strong>Journal:</strong> Journal of Appropriate Technology, Volume 8(2), 2022</p>
            <p><strong>DOI:</strong> <a href="https://www.e-jat.org/journal/view.php?doi=10.37675/jat.2022.00171" target="_blank">10.37675/jat.2022.00171</a></p>
        </div>
    
        <div class="paper-card">
            <h3>CAST: Cross-Attention in Space and Time for Video Action Recognition</h3>
            <p><strong>Authors:</strong> Dongho Lee, <b>Jongseo Lee</b>, Jinwoo Choi</p>
            <p><strong>Conference:</strong> NeurIPS 2023</p>
            <p><a href="https://neurips.cc/virtual/2023/poster/70748" target="_blank">Paper Link</a></p>
            <p><a href="https://github.com/KHU-VLL/CAST" target="_blank">Code</a></p>
        </div>
    
        <div class="paper-card">
            <h3>PCEvE: Part Contribution Evaluation Based Model Explanation for Human Figure Drawing Assessment and Beyond</h3>
            <p><strong>Authors:</strong> <b>Jongseo Lee</b>, Geo Ahn, Seong Tae Kim, Jinwoo Choi</p>
            <p><strong>Preprint:</strong> arXiv 2024 (under review)</p>
            <p><a href="https://arxiv.org/abs/2409.18260" target="_blank">Paper Link</a></p>
        </div>
    
        <div class="paper-card">
            <h3>Efficient Video Class Incremental Learning via Class Token Re-Learning</h3>
            <p><strong>Authors:</strong> <b>Jongseo Lee</b>, Suhyun Park, Jinwoo Choi</p>
            <p><strong>Conference:</strong> Korean Institute of Information Scientists and Engineers (KIISE), 2024</p>
            <p><a href="https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11862420" target="_blank">Paper Link</a></p>
        </div>
    
        <div class="paper-card">
            <h3>Audio-Video Cross Attention for Effective Video Action Recognition</h3>
            <p><strong>Authors:</strong> <b>Jongseo Lee</b>, Jang Juhyeon, Jinwoo Choi</p>
            <p><strong>Conference:</strong> Korean Institute of Information Scientists and Engineers (KIISE), 2024</p>
            <p><a href="https://www.dbpia.co.kr/Journal/articleDetail?nodeId=NODE11862452" target="_blank">Paper Link</a></p>
        </div>
    
    </div>
    



</body>
</html>
